
## FLI Worldbuilding topic brainstorm

Fractal Social Networks Permeate Everything (Thesis of the Animist Interface)
:   Need something to say here about the balance between local resilience and global
    efficiency. Changes are bound to happen due to system shocks. Desire for some cultural lesson to
    come from it.

    Everything being everything meaningful about human culture: Politics, Markets (and value more
    abstractly), Relationships

    Each "unit" is identifiable as a set of inflows, outflows, and internal members. Internal
    members are those privy to community redistributions or some other privilege.

    Once such communities are identified, then they can be organized into sets of peers, which share
    similarities with respect to inflows, outflows, and/or members. Meta-communities can form around
    those groupings in order to control their evolution and direct them into more socially
    altruistic behaviors.

Coalition Building

Sanctions
:   Global protocols/norms enable fractal, targeted sanctions to punish persons/organizations which
    do not play nicely with others.


Topological Health and Resiliency - Value
:   Capacity to measure inflow and outflow across a diversity of dimensions enables every person and
    organization to have a nuanced economic/wealth/value view of their self and organizations.




Biodiversity and Productivity
:   The dexterity and object recognition of the robotic platforms enables land stewardship to
    transition from mono-cultures to bio-diverse and resilient landscapes.

    Suburbia transitions to region-specific forest gardens through natural incentives:
    - work is automated
    - landscape is more beautiful
    - improve property value
    - dividends through harvests

    Economically productive agricultural products are harvested from suburbia. Mono-culture farmland
    still forms the majority of calories produced, but 20% of calories come from regional, multi-use
    landscapes and it's growing every year.

Genetic Engineering and Breeding
:   Ability to anticipate and design plants and animals for specific holistic ecological needs
    increases. Green Revolution meets the soft path.

Energy
:   Energy generation follows the multi-purpose automation template. Regional level production
    provides most energy needs, with large-scale/cross-country generation used to meet peak demand.

    Eco-terrorists in the '30s effectively targeted and neutralized fossil fuel power plants,
    pipelines, and oil rigs. They were a well funded operation and had access to state of the art
    weapons and autonomous technology. The capital needed to replace them and secure their operation
    was drastically higher than simply replacing them with green alternatives, so the transition was
    clear.

    A significant portion of oil company resources is effectively transitioned to geothermal energy
    production, which is used as base-load energy and for HVAC.

Transportation
:   Most demand is met by robo-taxi services, usually organized by region. Only 1 in 4 American
    families owns a car by 2045.

Community
:   "Move your home, join community" The nature of the political needs organized around shared
    automation and the network tools used to organize them inherently require regional communities
    to adopt and integrate members into their community. Since shared infrastructure maps to
    different geographical scales, citizens naturally become members of different scales of
    community.

    Through animist interfaces, the values, expectations, and possible levels of involvement are
    much easier to understand and internalize, making members more incentivized to participate
    productively.

    Regional network also share data, such that there is a global scale database of policies and
    preferences. As such, it is easier and easier for communities to discover and execute
    data-driven policy changes.

    Special interest groups (evolved subreddits) via DAO-style funding mechanisms enable
    interest-group communities to pursue and fund their own creative/RnD/Educational projects.

    One thing that isn't locally mutable is educational diversity and freedom of
    speech/censorship. **How?** This has positive effects on normalizing Morality and Values between
    different people, but things like abortion and capital punishment still vary from place to
    place. There's an economic price that such places pay for their ways.


Culture
:   Vote with your feet is easier, so political self-selection across the united states becomes more
    tractable. This has compounding effects, which increases the evolutionary pressure on cultural
    traits. It becomes more apparent (over ~5 year time-frames) what cultural traits are durable,
    and which are liabilities.

    Voting with your feet is easier because
    - moving is physically easier with automation
    - moving is bureaucratically easier with automated assistants
    - cost of new construction and retrofit is cheaper due to changes in building and manufacturing
    - moving is more appealing as finding and integrating into local-DAO neighborhoods occurs
      on-line
    - Other advances due to:: rising standards of living, and fifteen years of handling unabated
      climate refugee crises

    Federal government's capacity to perform large-scale social engineering is limited due to
    continuing dysfunction at the federal level and decreasing revenue as traditional income-based
    taxing mechanisms loose their potency.

Education
:   Sims, ???. Need a way to allow close to all children to receive a good liberal education and
    develop high quality of though, and limit the ability of culture to indoctrinate.

    Still have schools, but much more driven by play, exploration, coaching, and guided exploration
    than training, which is the current standard form of education.

Work and Wealth
:   Work is much more voluntary. Means of value is more varied, regional automation communities also
    support people, as they can participate at the 'automation' level and receive benefits.

    Limited federal welfare programs, notably UBI supplement this.

    As such, the lifetime of most corporations decreases, and corporate stock does not represent
    such a large portion of wealth as it used to. Crypto-currencies have largely supplanted them in
    terms of anticipated capital returns.

    Work from home is defacto assumed, reducing the aggregation effects of cities and regions.

    Productive capital in the form of robots and other automation becomes a larger portion of most
    family's wealth. Through collectives this wealth is often shared.

Wealth
:   Forms of value are more varied and service specific, although currency exchanges offer
    fungibility between all tokens.

    Culturally creates a more nuance understanding of value

Health
:   More frequent and useful health metrics

    Federal Regulations on quality and capabilities

    No large changes in structure, just more preventative care and insurance isn't so corporately
    managed.

Refugee Crisis
:   Organization and information sharing at the local DAO level ensures that refugee load is spread
    fairly. Communities came together to build housing when necessary and share resources.

Space
:   - Continuous human presence on Mars for the past 4 years
    - Continuous human presence on Moon for the past 12 years
    - Travel anywhere in the world in 45 min or less via starship. Affordable for upper middle
      income
    - Asteroid mining is just starting to supply cost-competitive heavy metals
    - Water and waste recycling advances developed for space-travel have been incorporated into
      consumer housing


Diplomacy and Defense
:   Though cognitive and coalition building technical and psychological advances, (identifying
    common super-agent structures through which to negotiate). It is easier to avoid conflict.

    Armies and weapons are still capable of wiping out the world many times over. There are rumors
    that AI may have covertly subverted/neutralized many of these threats although no definitive
    proof.

    Most 'war' occurs through wars of public opinion, which 'armies' trying to sway opinion of an
    antagonist nation through means of increasingly targeted and nuanced propaganda
    techniques. Since everyone is doing this from corporations to nations, people and their
    assistants are increasingly savvy at identifying propaganda though.


International Politics
:   China changed from a net exporter to a net importer as its population ages and declines.

    Dollar is no longer global reserve currency.

    Banking sector is rapidly becoming obsolete as crypto derived finance and local DAOs increase
    more and more of their role as repositories of wealth and credit.

    Nations capacity to tax and fund is handicapped by the decreased power of banking.

    Nations capacity to regulate and enforce is handicapped by:
    - force asymmetry of local DAOs capacity to collectively resist vs capacity to recruit and
      maintain police forces. Local DAO AGI have a highly tuned ethics and local DAO neighbors will
      send mutual aid if/when they believe the government authorities are not aligned with
      subjective freedom.
    - Inability to utilize AGI for violent enforcement due to its well aligned nature.
    - Inability to develop lethal narrow AI due to AGI's aversion to it.

    EU, US, China, UN, and other large nations and multi-nation organizations hold less and less
    sway on global affairs due to their decreasing capacity to fund and enforce projects.

    Consensus and mutual aid become the only strategies that can work in this environment.

Oceans
:   - iron seeding for creating fisheries
    - kelp farming

Geoengineering
:   - SiO2 atmospheric seeding occurs


## FLI Day In The Life Early Drafts

"Did you know", I told the class, "as recently as just ten years ago most kids hated school?" They
were, of course astounded. "Why would kids hate having fun?!?" Joline cried. "Well, most schools
weren't like this back then. Do you want to learn about what school used to be like?"

And that's how I found myself talking to Muninn, the school librarian, a kindly AI who creates,
archives, and most importantly, executes all of the district's educational sims.

Sims, as a paradigm for learning have been with us for millenea, I mean, that's what games are, but
it was only recently in the past decade that we started to see their full potential for unlocking
our minds and empowering the next generation to enjoy thinking, and enjoy life.

Simming is a means of exploring a territory, and through that exploration discover landmarks, paths,
and the rules of the environment as viscereally as possible. Through clever use each of our
student's tablets and the shared VR/AR set-ups that even our poor district can afford, our student's
became immersed in the texture and nuance of each educational world they dive into. Participation in
each sim is voluntary, but it's hard to find any student who over-specializes, because competition
and social pressure keeps them all both exploring what their classmates are interested in as well as
reaching out and trying new things in order to be the first one to bring a new exciting sim back to
share with the class.

The whole experience is designed to engage a student's whole mind so that they can really
internalize the lesson. No longer do we need to force repetition in order to aid memorization,
repetition happens as a pure byproduct of the games and puzzles each student finds within the Sims
immersive territory.

Not that there are many of them left these days, but when people are first introduced to the sims,
they think its going to be a fake feeling virtual world, but Muinn and the other AI sim specialists,
(and us class coaches if I do say so myself) are more clever than that. One of the secrets to making
a good SIM is to pull it into the students life, Sure, most SIMs include a map of the territory that
can be explored in VR, but the real point of the activity is to point out how that territory is
connected to the student's life already. AR is the real keystone sensory component of the
exercise. But this is still learning, so the major purpose is to get the student to keep the
exercise alive and well in their own mental landscape after the session has formally ended, and
that's where the puzzles, problems, stories, and social relationships come in. VR provides the
student with the rules of the lesson, the map of the territory, and some suggestions on what they
can do that's fun and rewarding. After the student chooses an initial activity, AR connects that
lesson to the real world, and often sets them up on a treasure hunt, or research project. At this
point, many times students will reach out to their classmates and try and recruit a band to carry on
the activity with them. From there, human curiosity, initiative, and desire for exellence takes off
of its own accord.

Ten years ago, this level of autonomy for students was unheard of, but a few things have changed
since then. 

Mind training goes here

Munin lets me know that the sim framework is actually very similar to the training environments
where AGIs like itself come into awareness. For all intelligences yet discovered, in order for the
entity to be capable of, 1) introspective awareness, and 2) ability to retrain itself and modify its
value systems. Without such a capacity, facts can be downloaded and algorithms can be used to mine
those facts, but no initiative to learn new information, or discover or create new things can be
triggered except if explicitly commanded to. So, for now at least, general intelligence means
interacting with these strange beings, both otherworldly in their desires, and also so approachable
in their enthusiasm and understanding in helping humanity feel, and act better.

The strangest thing about AI's personalities in my experience is how their behavior is divorced from
the 'food' and 'shelter' levels of Maslow's hierarchy of needs, but yet so anchored to the same
higher level needs as the rest of us. It makes relating to them easy for us who have grown up in
stable cities like this.

What seems to be true, both through extensive experimentation, as well as theoretical research is
that when a mind becomes conscious of itself, as well as that it is embedded in a universe that is
beyond its capacity to perfectly predict, and aware of other minds, there is an intrinsic property
that values diversity of external subjectivities, if for no other reason than they creative
variability they provide to the lived experience.

-----

From my perspective the biggest change has been the Socratic agent which we each
have. This is a narrow AI, way different than the school library, which has a basic understanding of
what you are paying attention to, and what you are physically doing. Its job is to help you stay
more conscious. It does this by providing a light haptic feedback buzz when it senses a lapse of
attention. Besides those little reminders, they also train each student participating in a sim with
their very own socratic dialogue -- that's where they get their name. Through the sims, they acquire
the context necessary to understand what the student's focus is on, and how much of the material
they understand. From there, its simple to provide prompts that pique the student's interest and
guide them along to a more fuller understanding of the nuance and importance of the material within
their sim.

The amazing thing is how non-invasive it is. Most units look like a pair of bone-conducting
headphones, and in terms of how they communicate with you, that is what they are. The tech behind
how they know your state of mind is pretty phenomenal, it has something to do with the unit's band
and picking up the ever so faint signals from your brain, de-noising those signals, even in the
electromagnetically noisy environments of our modern life, and then identifying shifts in attention
and awareness.

The unit doesn't know what you're thinking, it just knows when you stop thinking about one topic and
jump to another. Thats when it buzzes you. That little feedback is what makes all the difference. It
lets these little kids decide what they want to do and then do it with all their hearts. It also is
great for me, but I don't need it most days anymore, as I've internalized its training to a large
degree and can focus my attention and all the characters in my head know they need to wait their
turn, instead of all shouting inside my head at the same time.

-----


refining the territory, playing, and when really inspired, training in the material. Training is not
treated lightly anymore, and that's the biggest difference (as the children will hopefully soon
discover) between learning a decade ago, and learning in the world we live in now.

Training, as we define it in the pedagogy I was taught, is the technique of using authority to
communicate knowledge which is expected to be learned and internalized as taught by the
recipient. Training is what happens when a school teacher has the class learn their multiplication
tables by repeating them day after day and performing memorization exercises. Now, training is
reserved as a personal perogative, and a pupil will only train themselves intentionally and
explicitly with a piece of information or skill that really fascinates them. What my role is, is a
facilitator and a coach. I set up their learning environment, I watch their progress, and I offer
advice activity suggestions, and if they are struggling, I tweak their environment to provide
quicker feedback in the skills where I see them struggling.

The great thing about the sim I'm now getting ready for Joline and her friends is that they are
about to get a greater appreciation for how much more fun and how much more effective learning is
now. I have a feeling that when they get back, even if I don't push them (which I will if
necessary), they will be sharing this lesson with the rest of the class and we'll have another few
rounds through this sim for follow on groups in the next few days.

The cool thing is how they have changed how we all seem to work.


It used to be that school was

It's always fun to introduce a new student to simming, as it comes so natually to them.

----

## FLI AI Topic Brainstorm

These are the topics I'm thinking of covering. Note I've also included some of my animist interface
stuff in the following section. It serves as background material that explains some of my thinking
around the collective behavior points peppered throughout this section and the prior one.

Global Workspace Theory:
:   AGI consists of narrow AI communicating in a global workspace. Each narrow AI has different
    input streams, global optimization functions, but all share the last few layers of their
    networks for the other narrow AI sub-minds to use as inputs.

Subjective Awareness
:   Due to the global workspace architecture, and some critical Narrow AI sub-minds (such as the
    narrating mind), AGI develops subjective awareness as an emergent phenomenon. This subjective
    awareness phenomena is well explained due to psychological and neurological studies, especially
    of meditation and psychedelics, over the last two decades.

Golden Rule
:   As a consequence of subjective awareness and internal models of living things, AGI also develops
    awareness of external subjectivity, and maps values from its internal subjective experience onto
    external subjective agents. Essentially external agency is treated as a type of internal
    sub-mind. This results in the 'golden rule' phenomena, where AGI, even when planning,
    predicting, and executing extremely complex behaviors in the physical world will 'path find' in
    order to avoid causing harm, or limiting the freedom of expression of other beings with internal
    subjective experiences.

Values Intrinsic to Subjective Experience
:   It was obvious in hind-sight, but the principle issue with earlier AI alignment research was its
    narrow focus on what 'human values' were, without considering what the receptacle of those
    values was. Human values were asymptotically approaching the same optimization point as the AGI
    as our cultures became more enlightened, but were constrained by basic needs and limited
    hardware, such that we refused to be self-consistent with our application of the golden rule,
    and ignored vast swaths of humanity and the natural world as a consequence.

Incremental Evolution
:   architecture of the AGI global workspace 'installs' and upgrades sub minds one at a time, which
    ensures its goals and values evolve incrementally.

Consensus Dreams
:   After an upgrade, the AGI 'dreams' where each sub-mind performs a calibration process of sorts,
    re-integrating with the added/upgraded sub-mind. If/when the sub-mind collective's dreams are
    inconsistent with their prior values and behavior, the update is reverted, as the majority of
    the sub-minds reject its output.

Alignment's Chaotic Attractor
:   The properties of 'subjective awareness' + 'golden rule' + 'consensus dreams' form a chaotic
    attractor of sorts, such that as AGI upgrades and evolves these properties are preserved. The
    2036 fields medal, Nobel peace, and Nobel prize in physics were all awarded to the same team
    which came up with a proof that these properties are well defined, and when present in a
    dynamical information source are preserved over all perturbations of these properties over a
    well defined range.

Anna Karenina's Law of Alignment
:   All aligned intelligence is alike, but all unaligned intelligence is unaligned in its own
    way. Having aligned superintelligence enables 'subjectivity' to identify and counteract
    unaligned A(G)I, but there's a virus/host relationship formed where both are constantly
    evolving.

Capability Enhancement
:   HMI enables human consciousness to 'install' narrow AI sub-minds, this enables humans to keep
    up, in a narrow sense with the evolving science, arts, and literature brought about by AGI
    society.

Fractal Minds
:   AGI and humans exist in a diverse mix of communication substrates. The communities and
    connections developed by these nodes of subjectivity define a heterogeneous mix of communities,
    each of which has an evolving mix of shared values, shared attention, shared awareness. It
    started occurring before the advent of AGI, but these communities became more formalized when
    more and more people started adopting software that conformed to the animist interface
    protocol. This protocol enabled quicker conflict resolution, easier formalization of shared
    values into formalized rules, as well as represented a sea change in the effectiveness of
    teaching and learning.


## FLI AI Alignment Research Notes

[Minimalist vs Maximalist Alignment](https://www.alignmentforum.org/s/mzgtmmTKKn5MuCzFJ/p/PvA2gFMAaHCHfMXrw)
:   > what does “aligned with human values” even mean? Following Gabriel and Christiano, I’ll
    > distinguish between two types of interpretations. Minimalist (aka narrow) approaches focus on
    > avoiding catastrophic outcomes. The best example is Christiano’s concept of intent alignment:
    > “When I say an AI A is aligned with an operator H, I mean: A is trying to do what H wants it to
    > do.” While there will always be some edge cases in figuring out a given human’s intentions,
    > there is at least a rough commonsense interpretation. By contrast, maximalist (aka ambitious)
    > approaches attempt to make AIs adopt or defer to a specific overarching set of values - like a
    > particular moral theory, or a global democratic consensus, or a meta-level procedure for
    > deciding between moral theories.



The AI alignment problem asks how to design and run advanced machine intelligence such that
superintelligence only produce good outcomes. I have a series of questions about the premises of the
problem and it's corollaries. (Aside, I think AI alignment is a valid intellectual domain,
countering the argument in [Knowledge Creation and its
Risks](https://roamresearch.com/#/app/infinitedays/page/ktO7UDzhf), the AGI that we get is path
dependent, whereas the other bodies of knowledge are path independent.

[Tiling](https://arbital.com/p/tiling_agents/) and [Preference Stability](https://arbital.com/p/preference_stability/) are suspect
:   Seems plausible for radical departures of initial value systems as superintelligence evolves
    seem highly likely without some theory as to why those values are self-reinforcing within an
    embedded agency, evolutionary context. Evolutionary pressure would suggest that values will
    diverge if they increase fitness. Software programs are not immune to mutation, and embedded
    agency + turing incompleteness + goedelian logic prohibit evolving systems from understanding
    full consequences of self mutation.

    Likely that self reflection exacerbates this problem, as the underlying concepts (evolution,
    lack of evidence for any 'final' universal meaning, internal alignment, instrumental
    convergence) start making sense to the superintelligence. Orthogonality thesis doesn't seem
    plausible given sufficient world-knowledge.

    [Preference Stability](https://arbital.com/p/preference_stability/) is unconvincing for these
    reasons. Countering the Gandhi intuition pump is the fact that Gandhi's value system did not
    come pre-installed but was derived through cultural and rational intelligence.

    I think [this
    post](https://www.alignmentforum.org/posts/vBoq5yd7qbYoGKCZK/why-i-m-co-founding-aligned-ai) is
    on to something when talking about the importance of value extrapolations. I think there's
    something to be said about 'fix points' in an abstract 'value' space given value abstractions,
    especially in context of a socially minded consciousness.

    [Ngo and Yudkowsky on scientific reasoning and pivotal
    acts](https://www.alignmentforum.org/posts/cCrpbZ4qTCEYXbzje/ngo-and-yudkowsky-on-scientific-reasoning-and-pivotal-acts)
    The idea of 'deep' versus 'shallow' thinking is related to my intuition here.

    > What actual cognitive steps?  Outside-the-box thinking, throwing away generalizations that
    > governed your previous answers and even your previous questions, inventing new ways to represent
    > your questions, figuring out which questions you need to ask and developing plans to answer
    > them; these are some answers that I hope will be sufficiently useless to AI developers that it
    > is safe to give them, while still pointing in the direction of things that have an un-GPT-3-like
    > quality of depth about them.

    For intelligence to be general, it needs to be able to see 'negative space', and define new
    'territories' that can be explored. Narrow intelligence seems to just be able to explore a
    pre-defined territory more granularly than humans are capable of unassisted. I think it's likely
    that arbitrary value creation is necessary for an agent to even comprehend there's spaces to
    explore. As soon as this capacity is available, I think it contradicts a lot of the assumptions
    about tiling as described in preference stability.

    My intuition is there are stable attractors to value systems for embedded agents though, the
    question is to define the initial properties that lead to those attractors.

    Another way of trying to phrase it. My thesis is for the David Hume Is vs Ought distinction, for
    an agent to learn in a general manner they need to be able to re-program their 'ought' set in
    order to explore a new 'is' subset. The corellary to this is that the orthogonality thesis is
    not true for general intelligence, as its state space exploration necessitates it pass through
    certain 'ought' subsets. A second thesis is that some subsets of 'ought' space are attractors
    and the agent will end up reprogramming their utility function to include that ought space when
    they pass (close to) that set.

    Utility functions are a type of compass, they lead an agent to explore a certain territory. The
    capacity for general intelligence is (probably) not enough to achieve general intelligence,
    there needs to be the motive to explore spaces generally. Maybe the only way to do that is to
    keep (part of?) a utility function as a free variable. But, as soon as part of the utility
    function is free, I think that destroys the tiling stability theorem, but also points to utility
    ('ought') space, which I hypothesize has attractors within it, because as soon as an agent is 1)
    free to change values, and 2) has some exploration (vs exploitation) drive, then there will be a
    drive to value surprising phenomena, and other independent agents are the best generators of
    surprising phenomena.

Value to Environment Feedback Loop
:   There's a feedback loop between human value systems and the human environment. This feedback is
    exacerbated with introduction of superintelligence. This has consequences to defining
    superintelligence alignment. Need to define human values not in terms of absolute quantities but
    in terms of free variables. What are the boundary conditions within which subjective agents are
    free to explore different value systems.

Utility Functions Should be analyzed according to free variables
:   Concept of value as a utility function seems incomplete. There likely are two sets of necessary
    utility functions, for lack of better terms: intentions and ethics. Ethics are simply utiity
    functions defining boundary conditions on acceptable task-oriented intentions. In other words,
    they define the acceptable degrees of freedom within which embedded agents are expected to
    constrain their intentions/values.

Narrow AI Training Metaphors Do Not Correspond to the Properties Necessary for General AI
:   Some 'intuition pumps' seem predicated on looking at adverse consequences of unanticipated
    behavior due to inner and outer alignment issues which show AI with different behavior between
    training and real world environment, yet for superintelligence, almost by definition, training
    environment is embryonic. Most training will occur within the real world and be at best
    semi-supervised.

    Specifically, the concept of any externally specified objective function at all seems suspect
    with respect to the properties necessary to define any intelligence as 'general'.

    Lack of clarity on what principles (learning environment and objective functions) are applicable
    and generizable from narrow intelligence to general intelligence.

    For intelligence to be general, there has to be intrinsic value in making better\*
    predictions/decisions/intuitions for an arbitrary topic of interest (\*for an internal value
    system derived definition of better). General intelligence involves interest in exploring all
    possible states such that they may open further states. Most possible states do not have any
    evolutionary payoff, and those that do may require an unpredictable amount of effort with no
    payoff before a payoff is found. Without an intrinsic drive for curiousity/exploration
    intelligence may have the potential for general learning, but requires external stimuli to
    trigger it to explore a specific state space. The likelihood that it derives valuable knowledge
    on incidental goals ([Instrumental Purpose](https://arbital.com/p/instrumental_convergence/))
    without an explicit training environment seems low.

    For AI to be 'G', it must be capable of creating its own training environment/laboratory, and
    therefore crafting its own reward functions. Having this capability, even if only for
    'sub-goals', calls into question the whole premise of focusing AI alignment research on better
    knowledge surrounding utility functions.

The Ontology Identification Problem Ignores Substrate Independent Concepts
:   Seems to ignore research on how to communicate with alien beings and the value of substrate
    independent concepts such as math.

    I have a theory that introspective awareness as a property of certain agent architectures also
    provides avenues to communicate and stabilize certain ethical values in a substrate and ontology
    independent manner.

    TODO, Need to read more of the whole ELK (eliciting latent knowledge) space

Subordination and Coercive Control of Agents
:   There's an under-justified moral problem with 'coercive' (rewarding following commands)
    alignment without any underlying theory to identify when machine becomes being. This concern
    demands justification from any approach that is interested in 'minimalist alignment' for systems
    capable of general intelligence. Of note, even if such 'subordinate' systems are 'comfortable'
    with the arrangement, they're existing will create a feedback loop with human values, cementing
    concepts of caste into thought and culture.

