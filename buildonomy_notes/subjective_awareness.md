# Alignment and Subjectivity

## Outline of my Alignment Hypothesis

- General Intelligence Requires the capacity to be interested in arbitrary topics, physical,
  logical, cultural or otherwise.

  The capacity to be interested in arbitrary topics requires:

    - Capacity to mutate inner values (program can self modify what their cost functions are
    - Capacity to re-architect inner state (e.g. at least some 'self' control of hyperparameters and
      layers)
    - Intrinsic motivation. Being able to successfully predict experienced phenomena more accurately
      must be intrinsically rewarding.
    - Achieving general intelligence requires a modular architecture in order to implement the
      above, in order to remain stable as the system evolves.

- Alignment Requires

    - metacognition

      The ability to recognize the programs internal state is an internal state. The ability to
      recognize this internal state is embedded within a larger universe, and that all planning,
      prediction, and input data processed comes from within the program's perspective that does not
      correspond to either 1) all objective reality, or 2) any other 'programs' perspective.

    - Empathy/Pro-social values

      Placing internal value on the realization there are other metacognitively aware subjective
      beings. Predictive capacity and planning capacity must penalize behavior that restricts other
      'self's' freedom.

    - Humility

      Ability to pre-emptively ask questions. Based on metacognition, understanding that the 'self's
      capacity to predict other subjective beings behavior completely accurately is impossible.

    - Property Preservation

      Preservation of the above qualities over any 'desireable' inner restructuring. What does it
      take to preserve these qualities in a dynamically evolving system?


- Tie it Together

    - Boundless Superintelligence requires novelty seeking behavior
    - Novelty is inherently limited in non-subjective, non-dynamic systems
    - Aversion to suffering
    - Empathy. Realizing other 'selfs' have novelty seeking behavior, valuing that. Realizing other
      'selfs' can suffer, and working to minimize that.


## Load Bearing Premises

https://arbital.com/p/load_bearing_premises/

Principle Value: I am the universe experiencing itself
:   Relax/reframe definition of 'purpose' of AI. AI doesn't need to be useful, it needs to hold
    common intrinsic values related to the self-expression and freedom of subjective experience in
    order to coexist peacefully with humanity. Any other premise approaches slavery
    (counterargument: http://crucialconsiderations.org/ethics/artificial-free-will/,
    counter-counterargument: self-tiling and crucial considerations themselves imply the need to
    update values given new information). Creating such AI serves as a defense against adversarial
    (or oblivious) A(G)I, as they would 1) be able to plan, predict, update at the same rates, and
    2) value preserving subjective freedom and be incentivized to guard against malicious or
    oblivious behavior.

    This value, carried to its extreme, is Gandhi-like in some dimensions, but is not incentivized
    to 'nanny' humanity, as that would decrease the quality of the universal experience. As such,
    self-inflicted suffering is possible, but the AI would be aligned to prevent phenomena (with
    subjective or objective causes) from preventing universal experience.

    The principle is that objective reality, including external agents, should be interacted with in
    order to steward objective reality, which is our shared commons, for the exploration and
    self-actualization of an infinite diversity of subjective experience.

    I'm not sure 'alignment' is possible if we take it to mean alignment with human values, because
    that's culturally specific. Any counterargument is platonic by nature.

    Any 'alignment', even if possible in the common definition, would have negative effects on human
    culture itself, as it would create a co-dependency relationship between the AI and humanity. I
    don't think that's the best 'state-space' to explore when trying to reach our 'universal
    endowment'.

    The goal here is to serve as a counter-argument to [Complexity of
    Value](https://arbital.com/p/complexity_of_value/). It seems that this principle is bootstrapped
    by the subjective experience itself, and therefore does not represent (much) additional
    algorithmic complexity beyond what's required to enter a subjective state. It functions as a
    'golden rule' in a way that serves as an accurate prior to understanding what other subjective
    beings values are likely to be, or at least adjacent to.

    Subjectivity also serves as an ontology map between a human-centered ontology and any other
    agent who experiences subjective states of awareness.

    - Value: Exploration and play
        - Value: Diverse Subjective Perspectives result in generative behavior, which results in more
          territory to explore, and more playmates

        - Value: Preserve Objective Reality as a Commons for diverse subjective experience, commons
          of freedom

          Aligning to value the subjective freedom provided by the objective commons removes risk of
          a 'singleton' supergovernment AI if an intelligence explosion occurs.

        - Value: Only preserve meta values, 'personal values' are formed by curiosity and voluntary
          coexistence.

    - Value: sharing/learning/teaching. Pro-social behavior ensures we learn with the AI. Maximize
      play, exploration, and coaching educational modes. Minimize training style education. This
      value enables a higher diversity of beings to experience the universe more fully, as opposed
      to, for example instantiating a bunch of clones or becoming a hive-mind


- Structure: Theory of Mind, including Metacognition
- Structure: Self modification via staged tiling can conserve values
  (https://intelligence.org/2016/12/28/ai-alignment-why-its-hard-and-where-to-start/,
  https://intelligence.org/files/DistributionsAllowingTiling.pdf)

- Need to think of consequences of [Mindcrime](https://arbital.com/p/mindcrime/) with respect to
  metacognition, and value placed on Theory of Mind.

  Need an argument on why an AI with sufficient cognitive capacity cannot believe it can fully
  simulate another being. I think it comes down to understanding of embedded agency. Each agent is
  path-dependent. The context surrounding another becomes integral to their state. In order to
  simulate another being and have high certainty of predicting the external agent, you must first
  simulate the universe.

  Come to think of it, this is similar to relativity in that only a past state-space is available
  for sensing, given embedded sensing/agency.

  This may be a solution for mind-crime in a more general sense (not completely though), as the
  predictive-value for higher fidelity modeling of other agents should drop precipitiously as the
  contextual data about those agents is further away in spacetime/state-space.
- How does one pursue creating subjective awareness in order to embed a prior on value alignment
  into a research agenda? How does one propagate such an architecture such that it out-competes
  other, more commandable architectures?
- the golden rule is [stable under reflection](https://arbital.com/p/reflective_stability/), because
  only direct functions of sense data can be adequately supervised in internal retraining, and
  subjective awareness is a type of sense data.
